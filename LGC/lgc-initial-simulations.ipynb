{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys,os,time,pickle\n",
    "\n",
    "#numerical stuff\n",
    "import scipy as sp # is this necessary? \n",
    "import numpy as np # is this necessary?\n",
    "import networkx as nx # is this necessary? \n",
    "\n",
    "#plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import random as ra # is this necessary?\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import AgglomerativeClustering # is this necessary?\n",
    "from sklearn.cluster import KMeans # is this necessary?\n",
    "from sklearn.cluster import DBSCAN # is this necessary?\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#graspy and lgc\n",
    "from graspy.embed import *\n",
    "from graspy.simulations import sbm\n",
    "from graspy.cluster import GaussianCluster\n",
    "import localgraphclustering as lgc\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook investigates the performance of LGC and global spectral methods on a class of community detection tasks. In particular, assume $ A \\sim SBM(\\pi, n, B) $ where \n",
    "\n",
    "$$ B = \\begin{bmatrix} p & q & ... & q \\\\ \\vdots & \\ddots & & \\vdots\\\\ \\vdots & & \\ddots & \\vdots \\\\ q & ... & q & p \\end{bmatrix} $$\n",
    "\n",
    "and $ \\pi = \\frac{1}{K} \\begin{bmatrix} 1 & 1 & .. & 1 \\end{bmatrix} ^{T} $. Apart from the effects of $ p $ and $ q $ on the two clustering techniques, there are two important regimes we are interested in: $ K = O(1) $ and $ K = o(n) $. In the first part of the notebook we fix $ K $. In the second part we allow $ K $ to grow as $ log(n) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fixed, n grows, q is fixed \n",
    "\n",
    "K = 2 # Fix number of clusters to be 2\n",
    "nh = 500 # step size for the ns we consider\n",
    "ns = np.arange(500, 10000 + nh, step=nh) # sizes of graph to consider\n",
    "mc_its = 10 # number of iterations\n",
    "\n",
    "p = 0.5 # within-block probability of edge\n",
    "q = 0.1 # between block probability of edge\n",
    "B = np.array([\n",
    "    [p, q],\n",
    "    [q, p]\n",
    "])\n",
    "\n",
    "pi = (1/K) * np.ones(K) # class membership priors\n",
    "\n",
    "def experiment(n, pi, B, acorn=None):\n",
    "    if acorn is not None:\n",
    "        np.random.seed(acorn)\n",
    "        \n",
    "    n_clusters = len(pi)\n",
    "        \n",
    "    P = np.random.multinomial(n, pi) # multi-nomial trial; number of vertices in each cluster\n",
    "    Y = np.concatenate([i*np.ones(P[i]) for i in range(len(pi))]) # cluster \"labels\"\n",
    "    A = sbm(P, B) # random adjacency matrix\n",
    "    G = nx.from_numpy_array(A) # graph from adjacency matrix for lgc technique\n",
    "    nx.write_edgelist(G, \"sbm.edgelist\",data=False)\n",
    "    g = lgc.GraphLocal('sbm.edgelist','edgelist',' ') \n",
    "    \n",
    "    ASE = AdjacencySpectralEmbed() # initialize object (defaults?)\n",
    "    X_hat = ASE.fit_transform(A) # ASE(A)\n",
    "    \n",
    "    GMM = GaussianCluster(min_components=n_clusters, max_components=n_clusters) # initialize object\n",
    "    y = GMM.fit_predict(X_hat) # estimate communities\n",
    "    ari_ase = ari(y, Y) # how'd we do?!\n",
    "    \n",
    "    LSE = AdjacencySpectralEmbed() # initialize LSE object (defaults?)\n",
    "    X_hat = LSE.fit_transform(A) # LSE(A); matrix operations are in function \n",
    "    \n",
    "    GMM = GaussianCluster(min_components=n_clusters, max_components=n_clusters) # initialize object\n",
    "    y = GMM.fit_predict(X_hat) # estimate communities\n",
    "    ari_lse = ari(y, Y) # how'd we do?!\n",
    "    \n",
    "    # now local stuff, will require for loop over all nodes in graph..\n",
    "    ACLs = np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        seed = [i]\n",
    "        acl = lgc.approximate_PageRank(g,seed,normalize=False)[1] \n",
    "        ACLs[i] = acl\n",
    "        \n",
    "    #- \n",
    "    GMM_local = GaussianCluster(min_components=n_clusters,max_components=n_clusters) \n",
    "    y = GMM_local.fit_predict(ACLs) # naively cluster assuming gaussianity\n",
    "    ari_lgc = ari(y, Y) # how'd we do?!\n",
    "    \n",
    "    return ari_ase, ari_lse, ari_lgc\n",
    "\n",
    "def simulation(ns, k_function, p, q_function, mc_its, acorn=None, verbose=False):\n",
    "    ARIs = np.zeros((len(ns), 3, mc_its))\n",
    "    \n",
    "    for i, n in enumerate(ns):\n",
    "        # get number of clusters\n",
    "        if isinstance(k_function, int):\n",
    "            n_clusters=k_function\n",
    "        else:\n",
    "            n_clusters = int(k_function(n))\n",
    "\n",
    "        # assume equal probabilities for each cluster.. may want to change in future\n",
    "        pi = 1 / n_clusters * np.ones(n_clusters)\n",
    "\n",
    "        # get components of B matrix\n",
    "        if isinstance(q_function, float):\n",
    "            q = q_function\n",
    "        else:\n",
    "            q = q_function(n)\n",
    "\n",
    "        rows_of_B = [q*np.ones(n_clusters) for i in range(n_clusters)]\n",
    "        for j in range(n_clusters):\n",
    "            rows_of_B[j][j] = p\n",
    "\n",
    "        B = np.array(rows_of_B)\n",
    "\n",
    "        if verbose:\n",
    "            for j in tqdm(range(mc_its)):\n",
    "                ARIs[i, :, j] = experiment(n, pi, B, acorn)\n",
    "            \n",
    "        else:\n",
    "            for j in range(mc_its):\n",
    "                ARIs[i, :, j] = experiment(n, pi, B, acorn)\n",
    "            \n",
    "    mean_ARIs = np.mean(ARIs, axis=2)\n",
    "    stderr_ARIs = np.sqrt(np.var(ARIs, axis=2, ddof=1) / mc_its)\n",
    "    \n",
    "    return mean_ARIs, stderr_ARIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.00s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.85s/it]\u001b[A\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:06<00:06,  6.02s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.93s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "mean, stderr = simulation(ns = [50, 100], k_function=2, p=0.5, q_function=0.2, mc_its=2, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh",
   "language": "python",
   "name": "hh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
